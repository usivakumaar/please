<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Object Detection Camera Example</title>

</head>

<body>
  <h2>Object Detection Camera Example</h2>
  <p class="err" id="errorMessage"></p>
  <video id="videoInput" width=320 height=240 autoplay style="position: absolute;"></video>
  <canvas id="canvasOutput" width=320 height=240 style="z-index: 99999;position: absolute;"></canvas>
  <script>
    var errorCallback = function (e) {
      console.log('Reeeejected!', e);
    };

    // Not showing vendor prefixes.
    navigator.getUserMedia({ video: { facingMode: 'environment' }, audio: false }, function (localMediaStream) {
      var video = document.querySelector('video');
      video.src = window.URL.createObjectURL(localMediaStream);

      // Note: onloadedmetadata doesn't fire in Chrome when using it with getUserMedia.
      // See crbug.com/110938.
      video.onloadedmetadata = function (e) {
        // Ready to go. Do some stuff.
      };
    }, errorCallback);
  </script>
  <script src="adapter-5.0.4.js" type="text/javascript"></script>
  <script src="utils.js" type="text/javascript"></script>
  
  <script type="text/javascript">
    let utils = new Utils('errorMessage');
    

    let streaming = false;
    let videoInput = document.getElementById('videoInput');
    let startAndStop = document.getElementById('startAndStop');
    let canvasOutput = document.getElementById('canvasOutput');
    let canvasContext = canvasOutput.getContext('2d');



    function onVideoStarted() {
      streaming = true;
      startAndStop.innerText = 'Stop';
      videoInput.width = videoInput.videoWidth;
      videoInput.height = videoInput.videoHeight;
      utils.executeCode('codeEditor');
    }

    function onVideoStopped() {
      streaming = false;
      canvasContext.clearRect(0, 0, canvasOutput.width, canvasOutput.height);
      startAndStop.innerText = 'Start';
    }

    const FPS = 30;
    function processVideo() {
      try {
        
        var ctx=c.getContext("2d");
        let begin = Date.now();
        // start processing.
        cap.read(src);
        src.copyTo(dst);
        cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);
        // detect faces.
        classifier.detectMultiScale(gray, faces, 1.1, 3, 0);
        // draw faces.
        ctx.clearRect(0, 0, c.width, c.height);
        //ctx.clearRect(0, 0, c.width, c.height);
        //ctx.stroke();
        for (let i = 0; i < faces.size(); ++i) {
          
          console.log(faces.get(i));
          let face = faces.get(i);
          let point1 = new cv.Point(face.x, face.y);
          let point2 = new cv.Point(face.x + face.width, face.y + face.height);
          //cv.circle(dst,(point1,point2), 63, (0,0,255), -1);
          cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);
          //ctx.rect(face.x,face.y,150,100);
          //ctx.stroke();
          
        }
        
        cv.imshow('canvasOutput', dst);
        // schedule the next one.
        let delay = 1000 / FPS - (Date.now() - begin);
        setTimeout(processVideo, delay);
      } catch (err) {
          console.log(err);
      }
    };

    // schedule the first one.
    

    utils.loadOpenCv(() => {
      let faceCascadeFile = 'cascade.xml';
      utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () => {


        video = document.getElementById('videoInput');
        console.log(video);
        
        c=document.getElementById("canvasOutput");
        
        src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        dst = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        gray = new cv.Mat();
        cap = new cv.VideoCapture(video);
        faces = new cv.RectVector();
        classifier = new cv.CascadeClassifier();

        // load pre-trained classifiers
        classifier.load('cascade.xml');
        setTimeout(processVideo, 0);

      });
    });
  </script>
</body>

</html>